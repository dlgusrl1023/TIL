# 인공신경망

---

## 인공신경망이란

* 인공신경망이란 머신러닝의 학습 방법 중 가장 성능이 뛰어난 학습 방법이다.
  인공신경망이란 이름이 뜻하는 바와 같이 인공신경망은 인간의 신경 구조에 착안해서 나온 머신러닝 학습 방법이다.

---

## 인공신경망의 학습 방법

![퍼셉트론](https://t1.daumcdn.net/cfile/tistory/2221FC4A569DEA6026)

* 이때 x1 = 1, x2 = 2, x3= 3 이라고 하자. 
  화살표에 각각 w1,w2,w3 이 부여 됐다고 할 때,
  w1=0.5, w2=0.5, w3=0.5 라고 하자.
  이 때 출력값이 10이 되어야 한다고 할 때,
  x1*w1 + x2*w2 * x3*w3 = 4 < 10 이기 때문에 w1,w2,w3 값을 조절해서 학습하다보면
  w1 = 3, w2 = 2, w3 = 1인 경우에 10을 맞출 수 있다.
  
* |     |출력값 < 목표값|출력값 > 목표값|
  |-----|-----|-----|
  |뉴런>0|가중치 증가|가중치 감소|
  |뉴런<0|가중치 감소|가중치 증가|

* 이렇게 가중치를 변경하면서 목표값이 도달하는 과정이 학습이다.
  이처럼 지도학습은 목표값이 있고 이에 맞춰가는 과정이다.

* 학습률과 반응률는 가중치를 얼마나 감소/증가 시켜야하는지에 대한 척도이다.
  학습률이 1이면 1씩 변하고 2면 2씩 변한다. 보통 실전에서 인공지능 분야 종사자들은 이값을 0~0.5 정도로하고
  0.125정도가 적절하다.

### 인공신경망 학습

* 인공신경망의 학습은 입력데이터의 값을 연결해서 모형의 예측값을 생성하는 과정과 예측값과 실제값의 차이를 최소화하기 위해
  연결의 가중치를 갱신하는 학습과정(backpropagation)으로 나눠진다.
  대부분의 인공신경망 알고리즘은 가중치 갱신을 위해서 경사하강법(gradient descent)를 사용하는데,
  이는 비용함수를 가중치에 대해 편미분한 다음 가중치를 기울기 방향으로 조금씩 이동하는 과정을 반복함으로써 실제값과 예측값의 차이를 최소화하는 가중치를 찾는 학습법이다.
  
### 인공신경망 학습 절차

1. 예측하고자 하는 target에 따라 비용함수 R(T) 결정
2. 최초 가중치를 랜덤하게 입력하고, 예측값을 산출
3. 비용함수를 각각의 가중치에 대해서 편미분
4. r번째 가중치에서 학습률*편미분 만큼 더하거나 빼서, r+1번째 가중치 산출
5. 오차가 향상되지 않거나, 일정수준 이하로 떨어질 때까지 2-4과정 반복

### 인공신경망 학습 노하우

* 많은 데이터 준비

* 많은 파생변수를 생성

* 데이터 정제

* 분석대상을 신중히 선택

* 최척의 hidden layer와 neuron 결정

* 학습률을 조정해서 local minima을 수렴하는 것을 방지

* 모형 과적합을 방지하기 위해 다양한 기술 사용

---

## 인공지능에서 활성화 함수란

  * 활성화 함수란 어떤 신호를 입력 받아 이를 적절한 처리를 하여 출력해주는 함수이다.

  * input data -> {활성화 함수} -> output data 이렇게 생각하면 된다.

  * 선형함수인 h(x)=cx를 활성화함수로 사용한 3층 네트워크를 생각해볼때, 이를 식으로 나타내면 y(x)=h(h(h(x)))가 된다.
    이는 실은 y(x)=ax와 똑같은 식이다.(a=c^3). 즉, 은닉층이 없는 네트워크로 표현할 수 있다. 
    신경망에서 층을 쌓는 혜택을 얻고 싶다면 활성화함수로는 반드시 비선형 함수를 사용해야 한다.
    
  * 인공신경망 레이어의 결과값은 W*(Input) + b의 형태이기 때문에 linear하다. 여러 레이어를 겹친다고 해도 linear하다는 사실은 바뀌지 않는다. 
    Input에 대해서 linear 한 결과만 만들어낼 수 있는 모델은 필연적으로 좋은 모델이 될 수 없을 가능성이 많다. 
    레이어의 결과값에 non-linear한 Activation 함수를 적용하면, 그 결과값 func(W*(Input) +b)은 non-linear해진다. 
    따라서 전체 모델 자체도 non-linear 해지고 데이터를 더 잘 반영할 수 있는 모델이 생길 확률이 높아진다.
    
---

### 활성화 함수의 종류

1. step function

  * h(x) = 0 (x<=0)
  * h(x) = 1 (x>0)
  
1. sigmoid function

  * 이 함수는 항상 0과 1사이의 값만 가질 수 있도록 하는 비선형 함수이다.
  
  * S(t) = 1/(1+e^(-t))
 
1. ReLU function

  * sigmoid function의 Gradient Vanishing 문제를 해결하기 위해서 최근에 많이 사용되고 있는 활성화 함수 중 하나이다.

> Gradient Vanishing 문제란 0과 1사이의 값을 가지는 sigmoid fuction에서 아주 작은 값을 가질 경우 0에 매우 가까운 값을 가지게 된다.
  이 때 Gradient Descent를 사용해 Back-propagation 시 각 layer를 지니며 이를 지속적으로 곱해주는데, 이때 layer가 많을 경우에는 결국 0에 수렴한다.
  그렇기 때문에 layer가 많을 경우 sigmoid가 잘 작동하지 않는다
  
  * f(x) = 0 (x<0)
  * f(x) = x (x>=0)
  
 ---
