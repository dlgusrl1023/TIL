# 경사하강법

---

## 경사하강법이란

* 경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다.

  기본 아이디어는 함수의 기울기(경사)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜서 극값에 이를 때까지 반복시키는 것이다.
  
  
* 최적화할 함수 f(x) 에 대하여, 먼저 시작점 x0 를 정한다. 현재 x[i] 가 주어졌을 때, 그 다음으로 이동할 점인 x[i+1] 은 다음과 같이 계산된다.

  x[i+1] = x[i] - r[i] * gradient(x[i])

  이때 r[i] 는 이동할 거리를 조절하는 매개변수이다. 즉 딥러닝에서는 학습률을 의미한다.

  이 알고리즘의 수렴 여부는 f의 성질과 r[i] 의 선택에 따라 달라진다. 또한, 이 알고리즘은 로컬 미니멈으로 수렴한다.

  따라서 구한 값이 전역적인 최적해라는 것을 보장하지 않으며 시작점 x0 의 선택에 따라서 달라진다.

  이에 따라 다양한 시작점에 대해 하강법을 적용하여 그 중 가장 좋은 결과를 선택할 수도 있다.

---

## 딥러닝과 경사하강법

### 코스트 함수

* 코스트 함수란 가설 함수에 의해서 예측된 값과 트레이닝을 위해서 입력된 값 사이의 차이를 계산해주는 함수로,

  예측된 값과 입력된 값 들의 차이에 대한 평균 값을 구한다.
 
 
* 로지스틱 회귀에서 사용되는 코스트 함수는 다음과 같다.
  
  cost_function = (1/n)* Sum(-y_origin*log(sigmoid(Wx+b)) - (1-y_origin)*log(1-(sigmoid(Wx+b))))
  
  -> n 의 트레이닝 데이터의 수
  -> y_origin 은 트레이닝에 사용된 x에 대한 입력값


### 옵티마이져

* 코스트 함수의 최소값을 찾는 알고리즘을 옵티마이져라고 하는데 상황에 따라 여러 종류의 옵티마이져를 사용할 수 있다.
  
  이중에 하나로 사용되는 방법이 경사하강법이다.
  

### 머신 러닝의 순서

1. 학습 단계 - 모델을 만들기 위해서 실제 데이터를 수집하고, 데이터의 특징을 가지고 에측을 어떻게 할지 정의한 후, 가설을 정의하고
             이 가설을 기반으로 학습 시킨다.
             
  * 데이터 수집
  
  * 특징 정의
  
  * 가설 정의
  
  * 코스트 함수 정의
  
  * 학습
  
  
2. 예측 단계 - 학습이 끝나면 모델 (함수)가 주어지고, 예측은 단순하게, 모델에 값을 넣으면 학습된 모델에 의해서 결과값을 리턴해준다.












